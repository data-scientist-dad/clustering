# A Straightforward K-Means Clustering Example

This is an example of clustering customers from a wholesale customer database.  You can download the data used from the Berkley UCI Machine Learning Repository [here](https://archive.ics.uci.edu/ml/datasets/Wholesale+customers).

## 1. Install and Load Packages

Most of what I do involves tidyverse and tidy data, so I know I will use that.
If technical reasons don't make that practical, I may just install dplyr or specific parts
of the tidyverse that I need.  

I also often use ggplot and load that as a matter of course as well.
```{r Install and Load Packages, echo=FALSE}

install.packages("dplyr")
install.packages("ggplot2")

library("dplyr")
library("ggplot2")

```


## 2. Explore the Data

Start off by reading in the data and using the `summary()` command.
```{r Load the Data, echo=TRUE}

data <-read.csv("wholesale_customers_data.csv",header=T)

summary(data)

```
There is a big difference for the top customers in each category (e.g. Fresh goes from a min of 3 to a max of 112,151).  Normalizing / scaling the data won’t necessarily remove the outliers.  Doing a log transformation might help.   We could also remove those customers completely.  From a business perspective, you don’t really need a clustering algorithm to identify what your top customers are buying.  You usually need clustering and segmentation for your middle 50%.

## 3.  Prepare the Data

With that being said, let’s try removing the top 5 customers from each category.  We’ll use a custom function and create a new data set called data_rm_top
```{r Remove Outliers, echo=TRUE}

top_n_custs <- function (data,cols,n=5) { #Requires some data frame and the top N to remove
  
idx_to_remove <-integer(0) #Initialize a vector to hold customers being removed

for (c in cols){ # For every column in the data we passed to this function
col_order <-order(data[,c],decreasing=T) #Sort column "c" in descending order (bigger on top)
#Order returns the sorted index (e.g. row 15, 3, 7, 1, ...) rather than the actual values sorted.
idx <-head(col_order, n) #Take the first n of the sorted column C to
idx_to_remove <-union(idx_to_remove,idx) #Combine and de-duplicate the row ids that need to be removed
}
return(idx_to_remove) #Return the indexes of customers to be removed
}
top_custs <-top_n_custs(data,cols=3:8,n=5)

length(top_custs) #How Many Customers to be Removed?

data[top_custs,] #Examine the customers

data_rm_top<-data[-c(top_custs),] #Remove the Customers

```
## 3.  Conduct K-Means Clustering

Now, using data_rm_top, we can perform the cluster analysis.  
Important note: We’ll still need to drop the Channel and Region variables.  
These are two ID fields and are not useful in clustering.

```{r K-Means Clustering, echo=TRUE}

set.seed(76964057) #Set the seed for reproducibility

k <-kmeans(data_rm_top[,-c(1,2)], centers=5) #Create 5 clusters, Remove columns 1 and 2

k$centers #Display cluster centers

table(k$cluster) #Give a count of data points in each cluster

```
How do we interpret these results?  

* Cluster 1 looks to be a heavy Grocery and above average Detergents_Paper but low Fresh foods.

* Cluster 3 is dominant in the Fresh category.

* Cluster 5 might be either the “junk drawer” catch-all cluster or it might represent the small customers.

A measurement that is more relative would be the withinss and betweenss.

k$withinss would tell you the sum of the square of the distance from each data point to the cluster center.  Lower is better.  Seeing a high withinss would indicate either outliers are in your data or you need to create more clusters.

k$betweenss tells you the sum of the squared distance between cluster centers.  Ideally you want cluster centers far apart from each other.

It’s important to try other values for K.  You can then compare withinss and betweenss.  This will help you select the best K.   For example, with this data set, what if you ran K from 2 through 20 and plotted the total within sum of squares?  You should find an “elbow” point.  Wherever the graph bends and stops making gains in withinss you call that your K.

```{r Trying Different Values for K, echo=TRUE}

rng<-2:20 #K from 2 to 20

tries <-100 #Run the K Means algorithm 100 times

avg.totw.ss <-integer(length(rng)) #Set up an empty vector to hold all of points

for(v in rng){ # For each value of the range variable
 v.totw.ss <-integer(tries) #Set up an empty vector to hold the 100 tries
 for(i in 1:tries){
 k.temp <-kmeans(data.rm.top,centers=v) #Run kmeans
 v.totw.ss[i] <-k.temp$tot.withinss#Store the total withinss
 }
 avg.totw.ss[v-1] <-mean(v.totw.ss) #Average the 100 total withinss
}
plot(rng,avg.totw.ss,type="b", main="Total Within SS by Various K",
 ylab="Average Total Within Sum of Squares",
 xlab="Value of K")

```

This plot doesn’t show a very strong elbow.  Somewhere around K = 5 we start losing dramatic gains.  5 clusters seems like a reasonable selection.

This is a basic framework for doing K-means clustering in R.






